"use strict";(globalThis.webpackChunksourcescribe_docs=globalThis.webpackChunksourcescribe_docs||[]).push([[9971],{2399:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"api-reference/files/sourcescribe_api_openai_provider","title":"OpenAI GPT API Provider","description":"File Purpose and Overview","source":"@site/docs/api-reference/files/sourcescribe_api_openai_provider.md","sourceDirName":"api-reference/files","slug":"/api-reference/files/sourcescribe_api_openai_provider","permalink":"/sourcescribe-core/docs/api-reference/files/sourcescribe_api_openai_provider","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{}}');var r=s(4848),t=s(8453);const o={},a="OpenAI GPT API Provider",c={},d=[{value:"File Purpose and Overview",id:"file-purpose-and-overview",level:2},{value:"Main Components",id:"main-components",level:2},{value:"Class: <code>OpenAIProvider</code>",id:"class-openaiprovider",level:3},{value:"Key Functionality",id:"key-functionality",level:4},{value:"Dependencies and Imports",id:"dependencies-and-imports",level:2},{value:"Usage Examples",id:"usage-examples",level:2},{value:"Important Implementation Details",id:"important-implementation-details",level:2}];function l(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"openai-gpt-api-provider",children:"OpenAI GPT API Provider"})}),"\n",(0,r.jsx)(n.h2,{id:"file-purpose-and-overview",children:"File Purpose and Overview"}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"openai_provider.py"})," file contains the implementation of the ",(0,r.jsx)(n.code,{children:"OpenAIProvider"})," class, which is a provider for the OpenAI GPT API. This class is part of the ",(0,r.jsx)(n.code,{children:"sourcescribe-core"})," project and is used to interact with the OpenAI GPT language model for generating text responses."]}),"\n",(0,r.jsx)(n.h2,{id:"main-components",children:"Main Components"}),"\n",(0,r.jsxs)(n.h3,{id:"class-openaiprovider",children:["Class: ",(0,r.jsx)(n.code,{children:"OpenAIProvider"})]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"OpenAIProvider"})," class is the main component of this file. It inherits from the ",(0,r.jsx)(n.code,{children:"BaseLLMProvider"})," class and provides an implementation for generating responses using the OpenAI GPT API."]}),"\n",(0,r.jsx)(n.h4,{id:"key-functionality",children:"Key Functionality"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Initialization"}),": The ",(0,r.jsx)(n.code,{children:"__init__"})," method initializes the OpenAI provider by checking if the OpenAI SDK is installed and setting the API key, timeout, and base URL."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Response Generation"}),": The ",(0,r.jsx)(n.code,{children:"generate"})," method takes a list of ",(0,r.jsx)(n.code,{children:"LLMMessage"})," objects, an optional system prompt, and additional parameters, and generates a response using the OpenAI GPT API. It converts the input messages to the OpenAI format, makes the API call, and returns an ",(0,r.jsx)(n.code,{children:"LLMResponse"})," object."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Streaming Response Generation"}),": The ",(0,r.jsx)(n.code,{children:"generate_streaming"})," method is similar to ",(0,r.jsx)(n.code,{children:"generate"}),", but it generates the response in a streaming fashion, yielding response chunks as they become available."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"dependencies-and-imports",children:"Dependencies and Imports"}),"\n",(0,r.jsx)(n.p,{children:"The file imports the following modules and types:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"typing"}),": For type annotations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"sourcescribe.api.base"}),": For the ",(0,r.jsx)(n.code,{children:"BaseLLMProvider"}),", ",(0,r.jsx)(n.code,{children:"LLMMessage"}),", and ",(0,r.jsx)(n.code,{children:"LLMResponse"})," classes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"openai"}),": The OpenAI SDK, which is imported conditionally based on its availability"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"usage-examples",children:"Usage Examples"}),"\n",(0,r.jsxs)(n.p,{children:["Here's an example of how to use the ",(0,r.jsx)(n.code,{children:"OpenAIProvider"})," class:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'from sourcescribe.api.openai_provider import OpenAIProvider\nfrom sourcescribe.api.base import LLMMessage\n\nprovider = OpenAIProvider(api_key="your_openai_api_key")\n\nmessages = [\n    LLMMessage(role="user", content="What is the capital of France?"),\n    LLMMessage(role="assistant", content="The capital of France is Paris."),\n]\n\nresponse = provider.generate(messages)\nprint(response.content)  # Output: "The capital of France is Paris."\n'})}),"\n",(0,r.jsx)(n.h2,{id:"important-implementation-details",children:"Important Implementation Details"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"API Key Validation"}),": The ",(0,r.jsx)(n.code,{children:"OpenAIProvider"})," class checks if the ",(0,r.jsx)(n.code,{children:"OPENAI_API_KEY"})," environment variable is set and raises a ",(0,r.jsx)(n.code,{children:"ValueError"})," if it is not."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Default Model"}),": If the ",(0,r.jsx)(n.code,{children:"model"})," parameter is not provided, the class sets the default model to ",(0,r.jsx)(n.code,{children:'"gpt-4-turbo-preview"'}),"."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Message Conversion"}),": The ",(0,r.jsx)(n.code,{children:"generate"})," and ",(0,r.jsx)(n.code,{children:"generate_streaming"})," methods convert the input ",(0,r.jsx)(n.code,{children:"LLMMessage"})," objects to the format expected by the OpenAI API."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"API Call"}),": The methods make the API call to the OpenAI GPT API using the ",(0,r.jsx)(n.code,{children:"openai.chat.completions.create"})," method, passing in the appropriate parameters."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Response Handling"}),": The methods return an ",(0,r.jsx)(n.code,{children:"LLMResponse"})," object, which contains the generated text, model information, usage statistics, and the raw API response."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Streaming Response"}),": The ",(0,r.jsx)(n.code,{children:"generate_streaming"})," method uses the ",(0,r.jsx)(n.code,{children:"stream=True"})," parameter to generate the response in a streaming fashion, yielding response chunks as they become available."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Context Overflow Prevention"}),": The code includes a comment indicating that the content is limited to prevent context overflow, but the implementation details are not provided in the given code snippet."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var i=s(6540);const r={},t=i.createContext(r);function o(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);